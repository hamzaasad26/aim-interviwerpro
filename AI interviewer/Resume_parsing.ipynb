{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install PyMuPDF\n",
    "pip install python-dotenv\n",
    "pip install llama_parse\n",
    "pip install llama-index\n",
    "pip install groq\n",
    "pip install nest_asyncio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # also known as PyMuPDF\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from groq import Groq\n",
    "import json\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 4630b56c-7f5b-4aff-b87c-23f4dd46b5ec\n",
      "Parsed Content: # Ak siIu\n",
      "\n",
      "Nal )15 1 76\n",
      "\n",
      "Haider Farooq\n",
      "\n",
      "Mobile: +92 317 4008222 1234haiderfarooq@gmail.com\n",
      "\n",
      "Islamabad Pakistan https://github.com/haiderfarooq3\n",
      "\n",
      "# Education\n",
      "\n",
      "FAST National University of Computer and Emerging Sciences\n",
      "\n",
      "Aug 2022 – Present\n",
      "\n",
      "Majoring in BS (Data Science)\n",
      "\n",
      "Relevant Courses: Programming Fundamentals, Object Oriented Programming, Data Structures, Introduction to Data Science, Fundamentals of Big Data Analytics, Advanced Statistics, Computer Organisation and Assembly Language\n",
      "\n",
      "# Projects\n",
      "\n",
      "|Spotify Clone – Python|May 2024|\n",
      "|---|---|\n",
      "|- Developed a Spotify alternative with music recommendation, playback, and streaming using a sample of the FMA dataset, Python for feature extraction, MongoDB for data storage, and K-means clustering for model training.\n",
      "- Deployed an interactive music streaming web app with real-time recommendations using Flask and Apache Kafka, showcasing personalized music recommendations and real-time streaming.\n",
      "| |\n",
      "|Automated Kafka Setup for Real-Time Product Insights|April 2024|\n",
      "|- Extracted insights from Amazon metadata using Apriori and PCY algorithms, real-time data streaming, and MongoDB integration for scalable analysis.\n",
      "- Automated Kafka setup with a bash script, showcasing the potential of streaming analytics for actionable product insights.\n",
      "| |\n",
      "|Text Mining and Analysis Pipeline|March 2024|\n",
      "|- Text Data Processing and TF-IDF Vector Generation: Cleaned and standardized textual data using NLTK, generated TF-IDF vectors, and normalized them using NumPy to ensure effective document comparison.\n",
      "- Scalable Analysis with Hadoop: Utilized Hadoop clusters for scalable data processing, implementing MapReduce to calculate word frequencies, IDF values, and TF-IDF scores, enabling efficient analysis of large datasets.\n",
      "| |\n",
      "|Advanced Image Retrieval System using LSH|March 2024|\n",
      "|- Image Processing and LSH Implementation: Preprocessed CIFAR-10 images, performed color analysis, generated textual shingles, and used Minhashing and Locality-Sensitive Hashing (LSH) for efficient similarity detection.\n",
      "- Web Application Integration: Built a Flask web app to handle image uploads and display similar images using the LSH algorithm, ensuring fast, scalable retrieval and improved user engagement.\n",
      "| |\n",
      "|Sentiment Analysis and Preprocessing of Customer Reviews|Feb 2024|\n",
      "|- Text Preprocessing: Loads reviews from a JSON file, removes stop words and punctuation, and converts text to lowercase using a comprehensive stop words list.\n",
      "- Sentiment Analysis: Analyzes reviews with predefined word sets, categorizes sentiment, and saves results with sentiment scores to a text file.\n",
      "| |\n",
      "\n",
      "# Additional Skills and Experience\n",
      "\n",
      "- ApacheHadoop, Kafka & Spark (Beginner Level)\n",
      "- Machine Learning\n",
      "- PowerBI\n",
      "- MongoDB\n",
      "- C++\n",
      "- SFML\n",
      "- HTML\n",
      "- Assembly Language\n",
      "Resume Data: {'personal_info': {'email': '1234haiderfarooq@gmail.com', 'phone': '+92 (317) 400-8222'}, 'education': {}, 'experience': 'Experience\\n\\n- ApacheHadoop, Kafka & Spark (Beginner Level)\\n- Machine Learning\\n- PowerBI\\n- MongoDB\\n- C++\\n- SFML\\n- HTML\\n- Assembly Language', 'skills': 'Skills and Experience\\n\\n- ApacheHadoop, Kafka & Spark (Beginner Level)\\n- Machine Learning\\n- PowerBI\\n- MongoDB\\n- C++\\n- SFML\\n- HTML\\n- Assembly Language', 'projects': '', 'certifications': '', 'extra_curricular_activities': ''}\n",
      "Here is the formatted output based on the provided resume text:\n",
      "\n",
      "**Personal Information**\n",
      "* Name: Haider Farooq\n",
      "* Email: 1234haiderfarooq@gmail.com\n",
      "* Phone: +92 317 4008222\n",
      "* LinkedIn: (not provided)\n",
      "* GitHub: https://github.com/haiderfarooq3\n",
      "\n",
      "**Education**\n",
      "* Degree: BS (Data Science)\n",
      "* University: FAST National University of Computer and Emerging Sciences\n",
      "* Graduation Date: (Expected August 2024)\n",
      "* GPA: (Not provided)\n",
      "* Courses: Programming Fundamentals, Object Oriented Programming, Data Structures, Introduction to Data Science, Fundamentals of Big Data Analytics, Advanced Statistics, Computer Organisation and Assembly Language\n",
      "\n",
      "**Experience**\n",
      "* None provided\n",
      "\n",
      "**Skills**\n",
      "* Programming Languages: Python, C++\n",
      "* Frameworks: Apache Kafka, Flask, Apache Spark, SFML\n",
      "* Tools: MongoDB, Power BI, Hadoop, Assembly Language\n",
      "\n",
      "**Projects**\n",
      "* Spotify Clone – Python\n",
      "\t+ Developed a Spotify alternative with music recommendation, playback, and streaming using a sample of the FMA dataset, Python for feature extraction, MongoDB for data storage, and K-means clustering for model training.\n",
      "\t+ Deployed an interactive music streaming web app with real-time recommendations using Flask and Apache Kafka, showcasing personalized music recommendations and real-time streaming.\n",
      "* Automated Kafka Setup for Real-Time Product Insights\n",
      "\t+ Extracted insights from Amazon metadata using Apriori and PCY algorithms, real-time data streaming, and MongoDB integration for scalable analysis.\n",
      "\t+ Automated Kafka setup with a bash script, showcasing the potential of streaming analytics for actionable product insights.\n",
      "* Text Mining and Analysis Pipeline\n",
      "\t+ Text Data Processing and TF-IDF Vector Generation: Cleaned and standardized textual data using NLTK, generated TF-IDF vectors, and normalized them using NumPy to ensure effective document comparison.\n",
      "\t+ Scalable Analysis with Hadoop: Utilized Hadoop clusters for scalable data processing, implementing MapReduce to calculate word frequencies, IDF values, and TF-IDF scores, enabling efficient analysis of large datasets.\n",
      "* Advanced Image Retrieval System using LSH\n",
      "\t+ Image Processing and LSH Implementation: Preprocessed CIFAR-10 images, performed color analysis, generated textual shingles, and used Minhashing and Locality-Sensitive Hashing (LSH) for efficient similarity detection.\n",
      "\t+ Web Application Integration: Built a Flask web app to handle image uploads and display similar images using the LSH algorithm, ensuring fast, scalable retrieval and improved user engagement.\n",
      "* Sentiment Analysis and Preprocessing of Customer Reviews\n",
      "\t+ Text Preprocessing: Loads reviews from a JSON file, removes stop words and punctuation, and converts text to lowercase using a comprehensive stop words list.\n",
      "\t+ Sentiment Analysis: Analyzes reviews with predefined word sets, categorizes sentiment, and saves results with sentiment scores to a text file.\n",
      "\n",
      "**Certifications**\n",
      "* None provided\n",
      "\n",
      "**Extra-Curricular Activities**\n",
      "* None provided\n"
     ]
    }
   ],
   "source": [
    "# give file path at the end \n",
    "# creates a text file and formats extracted daat to save it into the text file.\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "nest_asyncio.apply()\n",
    "# Initialize LlamaParse and Groq\n",
    "parser = LlamaParse(result_type=\"markdown\")\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_info(text):\n",
    "    info = {}\n",
    "    # Extract email\n",
    "    email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    if email_match:\n",
    "        info['email'] = email_match.group(0)\n",
    "    # Extract LinkedIn\n",
    "    linkedin_match = re.search(r'linkedin\\.com/in/[\\w-]+', text)\n",
    "    if linkedin_match:\n",
    "        info['linkedin'] = linkedin_match.group(0)\n",
    "    # Extract CGPA\n",
    "    gpa_match = re.search(r'\\bCGPA[:\\s]+([0-4]\\.\\d{1,2})\\b', text, re.IGNORECASE)\n",
    "    if gpa_match:\n",
    "        info['cgpa'] = gpa_match.group(1)\n",
    "    # Extract expected graduation\n",
    "    grad_match = re.search(r'Expected Graduation[:\\s]+([A-Za-z]+\\s\\d{4})\\b', text, re.IGNORECASE)\n",
    "    if grad_match:\n",
    "        info['expected_graduation'] = grad_match.group(1)\n",
    "    # Extract phone number\n",
    "    phone_match = re.search(r'\\b(?:\\+?(\\d{1,3}))?[\\s.-]?\\(?(\\d{3})\\)?[\\s.-]?(\\d{3})[\\s.-]?(\\d{4})\\b', text)\n",
    "    if phone_match:\n",
    "        info['phone'] = f\"+{phone_match.group(1) or ''} ({phone_match.group(2)}) {phone_match.group(3)}-{phone_match.group(4)}\"\n",
    "    return info\n",
    "\n",
    "def extract_experience_section(text):\n",
    "    experience_section_match = re.search(r'EXPERIENCE[\\s\\S]+?(?=\\n[A-Z]|$)', text, re.IGNORECASE)\n",
    "    if experience_section_match:\n",
    "        return experience_section_match.group(0).strip()\n",
    "    return \"\"\n",
    "\n",
    "def extract_skills_section(text):\n",
    "    skills_section_match = re.search(r'SKILLS[\\s\\S]+?(?=\\n[A-Z]|$)', text, re.IGNORECASE)\n",
    "    if skills_section_match:\n",
    "        return skills_section_match.group(0).strip()\n",
    "    return \"\"\n",
    "\n",
    "def format_experience(experience_text):\n",
    "    formatted_experience = \"Experience\\n\"\n",
    "    lines = experience_text.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:  # Only add non-empty lines\n",
    "            formatted_experience += f\"* {line}\\n\"\n",
    "    return formatted_experience.strip() if formatted_experience != \"Experience\\n\" else \"Experience: NULL\"\n",
    "\n",
    "def format_skills(skills_text):\n",
    "    formatted_skills = \"Skills\\n\"\n",
    "    lines = skills_text.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:  # Only add non-empty lines\n",
    "            formatted_skills += f\"* {line}\\n\"\n",
    "    return formatted_skills.strip() if formatted_skills != \"Skills\\n\" else \"Skills: NULL\"\n",
    "\n",
    "def process_resume_with_llama_and_groq(pdf_path):\n",
    "    # Initialize LlamaParse\n",
    "    file_extractor = {\".pdf\": parser}\n",
    "    documents = SimpleDirectoryReader(input_files=[pdf_path], file_extractor=file_extractor).load_data()\n",
    "\n",
    "    if not documents:\n",
    "        print(\"No documents were parsed. Please check the file path and format.\")\n",
    "        return\n",
    "\n",
    "    # Extract and print the parsed content\n",
    "    parsed_content = documents[0].text  # Access the text directly\n",
    "    print(f\"Parsed Content: {parsed_content}\")\n",
    "\n",
    "    # Initialize the dictionary\n",
    "    resume_data = {\n",
    "        \"personal_info\": extract_info(parsed_content),\n",
    "        \"education\": {},  # Extract education info here if needed\n",
    "        \"experience\": extract_experience_section(parsed_content),\n",
    "        \"skills\": extract_skills_section(parsed_content),\n",
    "        \"projects\": \"\",  # Extract projects info here if needed\n",
    "        \"certifications\": \"\",  # Extract certifications info here if needed\n",
    "        \"extra_curricular_activities\": \"\"  # Extract extra-curricular activities info here if needed\n",
    "    }\n",
    "\n",
    "    # Print the dictionary for debugging\n",
    "    print(\"Resume Data:\", resume_data)\n",
    "\n",
    "    # Save each section to a file if needed\n",
    "    with open('resume_data.json', 'w') as file:\n",
    "        json.dump(resume_data, file, indent=4)\n",
    "\n",
    "    # Optionally use Groq for additional formatting\n",
    "    prompt = f\"\"\"\n",
    "    Extract and format the following information from the resume text:\n",
    "\n",
    "    {parsed_content}\n",
    "\n",
    "    Format the output in the following manner:\n",
    "\n",
    "    **Personal Information**\n",
    "    * Name: \n",
    "    * Email: \n",
    "    * Phone: \n",
    "    * LinkedIn: \n",
    "    * GitHub: \n",
    "\n",
    "    **Education**\n",
    "    * Degree: \n",
    "    * University: \n",
    "    * Graduation Date: \n",
    "    * GPA: \n",
    "    * Courses: \n",
    "\n",
    "    **Experience**\n",
    "    * Job Title: \n",
    "    * Company: \n",
    "    * Location: \n",
    "    * Dates: \n",
    "    * Responsibilities: \n",
    "\n",
    "    **Skills**\n",
    "    * Programming Languages: \n",
    "    * Frameworks: \n",
    "    * Tools: \n",
    "\n",
    "    **Projects**\n",
    "    * Project Name: \n",
    "    * Description: \n",
    "\n",
    "    **Certifications**\n",
    "    * Certification: \n",
    "\n",
    "    **Extra-Curricular Activities**\n",
    "    * Activity: \n",
    "\n",
    "    Please ensure all sections are included, even if some might be empty. Format each section clearly and include relevant details.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a completion request with Groq\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    # Print the formatted resume information from Groq\n",
    "    formatted_resume_from_groq = chat_completion.choices[0].message.content\n",
    "    print(formatted_resume_from_groq)\n",
    "\n",
    "    # Save the formatted resume content to a file\n",
    "    with open('formatted_resume_from_groq.txt', 'w') as file:\n",
    "        file.write(formatted_resume_from_groq)\n",
    "\n",
    "#FILE PATH FOR RESUME\n",
    "pdf_path = 'tests/HaiderFarooq_CV .pdf'\n",
    "process_resume_with_llama_and_groq(pdf_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
